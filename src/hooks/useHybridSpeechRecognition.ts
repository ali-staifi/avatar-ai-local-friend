import { useRef, useEffect, useState, useCallback } from 'react';
import { toast } from 'sonner';
import { voskModelManager, ModelLoadingProgress } from '@/services/VoskModelManager';
import { useVoiceActivityDetection } from './useVoiceActivityDetection';

export type SpeechEngine = 'web-speech' | 'vosk';
export type SupportedLanguage = 'fr' | 'ar';

interface HybridSpeechConfig {
  engine: SpeechEngine;
  language: SupportedLanguage;
  continuous?: boolean;
  interimResults?: boolean;
  vadEnabled?: boolean; // Nouvelle option pour VAD
}

interface SpeechRecognitionEvent {
  results: {
    [index: number]: {
      [index: number]: {
        transcript: string;
      };
    };
  };
}

interface SpeechRecognitionInstance {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  start(): void;
  stop(): void;
  onresult: ((event: SpeechRecognitionEvent) => void) | null;
  onerror: (() => void) | null;
  onend: (() => void) | null;
}

export const useHybridSpeechRecognition = (
  onResult: (transcript: string) => void,
  config: HybridSpeechConfig = { engine: 'web-speech', language: 'fr', vadEnabled: true }
) => {
  const [isListening, setIsListening] = useState(false);
  const [currentEngine, setCurrentEngine] = useState<SpeechEngine>(config.engine);
  const [currentLanguage, setCurrentLanguage] = useState<SupportedLanguage>(config.language);
  const [modelProgress, setModelProgress] = useState<ModelLoadingProgress[]>([]);
  const [engineStatus, setEngineStatus] = useState<'ready' | 'loading' | 'error'>('ready');
  const [vadEnabled, setVadEnabled] = useState(config.vadEnabled ?? true);
  
  const webSpeechRef = useRef<SpeechRecognitionInstance | null>(null);
  const voskWorkerRef = useRef<Worker | null>(null);

  // VAD integration pour amÃ©liorer la reconnaissance
  const {
    isInitialized: vadInitialized,
    isListening: vadListening,
    bufferStatus,
    startListening: startVAD,
    stopListening: stopVAD,
    vadSupported
  } = useVoiceActivityDetection({
    enabled: vadEnabled,
    sampleRate: 16000,
    frameSize: 30,
    aggressiveness: 2,
    bufferDuration: 3000, // 3 secondes de buffer
    silenceThreshold: 800, // 800ms de silence
    voiceThreshold: 300, // 300ms de voix
    onVoiceSegmentDetected: (audioSegment: Float32Array) => {
      console.log(`ðŸŽ¯ VAD: Segment vocal dÃ©tectÃ© (${audioSegment.length} Ã©chantillons)`);
      // Ici on pourrait envoyer le segment directement Ã  Vosk ou Whisper
      // Pour cette dÃ©mo, on simule une transcription
      if (currentEngine === 'vosk' && audioSegment.length > 0) {
        simulateVoskTranscription(audioSegment);
      }
    }
  });

  // Surveiller les progrÃ¨s de chargement des modÃ¨les Vosk
  useEffect(() => {
    const handleProgress = (progress: ModelLoadingProgress[]) => {
      setModelProgress(progress);
    };

    voskModelManager.onProgressUpdate(handleProgress);
    return () => voskModelManager.offProgressUpdate(handleProgress);
  }, []);

  // Initialiser Web Speech API
  useEffect(() => {
    if (currentEngine === 'web-speech') {
      if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
        const SpeechRecognitionConstructor = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
        webSpeechRef.current = new SpeechRecognitionConstructor();
        
        const recognition = webSpeechRef.current;
        recognition.continuous = config.continuous || false;
        recognition.interimResults = config.interimResults || false;
        recognition.lang = currentLanguage === 'fr' ? 'fr-FR' : 'ar-SA';

        recognition.onresult = (event: SpeechRecognitionEvent) => {
          const transcript = event.results[0][0].transcript;
          onResult(transcript);
          setIsListening(false);
          // ArrÃªter VAD si actif
          if (vadEnabled && vadListening) {
            stopVAD();
          }
        };

        recognition.onerror = () => {
          setIsListening(false);
          setEngineStatus('error');
          if (vadEnabled && vadListening) {
            stopVAD();
          }
          toast.error("Erreur de reconnaissance vocale Web Speech", {
            description: "Impossible de capturer l'audio. Essayez Vosk avec VAD."
          });
        };

        recognition.onend = () => {
          setIsListening(false);
          if (vadEnabled && vadListening) {
            stopVAD();
          }
        };

        setEngineStatus('ready');
      } else {
        setEngineStatus('error');
        toast.error("Web Speech API non supportÃ©e", {
          description: "Votre navigateur ne supporte pas Web Speech API. Utilisez Vosk avec VAD."
        });
      }
    }
  }, [currentEngine, currentLanguage, config.continuous, config.interimResults, onResult, vadEnabled, vadListening, stopVAD]);

  // Simuler la transcription Vosk avec les segments VAD
  const simulateVoskTranscription = useCallback((audioSegment: Float32Array) => {
    // En production, on enverrait le segment audio Ã  Vosk
    console.log(`ðŸŽ¤ Vosk: Traitement segment (${(audioSegment.length / 16000).toFixed(2)}s)`);
    
    const mockResults = {
      fr: [
        "Bonjour, comment allez-vous ?", 
        "Pouvez-vous m'aider avec Ã§a ?", 
        "C'est parfait, merci beaucoup",
        "Je voudrais savoir comment faire",
        "D'accord, je comprends maintenant"
      ],
      ar: [
        "Ù…Ø±Ø­Ø¨Ø§ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ", 
        "Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠØŸ", 
        "Ø´ÙƒØ±Ø§ Ø¬Ø²ÙŠÙ„Ø§ Ù„Ùƒ",
        "Ø£Ø±ÙŠØ¯ Ø£Ù† Ø£Ø¹Ø±Ù ÙƒÙŠÙ Ø£ÙØ¹Ù„ Ù‡Ø°Ø§",
        "Ø­Ø³Ù†Ø§ØŒ Ø£ÙÙ‡Ù… Ø§Ù„Ø¢Ù†"
      ]
    };
    
    // Simuler un dÃ©lai de traitement rÃ©aliste
    setTimeout(() => {
      const randomResult = mockResults[currentLanguage][Math.floor(Math.random() * mockResults[currentLanguage].length)];
      console.log(`âœ… Vosk: Transcription via VAD: "${randomResult}"`);
      onResult(randomResult);
      setIsListening(false);
      
      toast.success("Transcription VAD+Vosk", {
        description: `Segment analysÃ© automatiquement: ${(audioSegment.length / 16000).toFixed(1)}s`
      });
    }, 500 + Math.random() * 1000);
  }, [currentLanguage, onResult]);

  const initializeVosk = useCallback(async () => {
    if (currentEngine !== 'vosk') return;

    try {
      setEngineStatus('loading');
      
      if (!voskModelManager.isModelLoaded(currentLanguage)) {
        toast.info(`Chargement du modÃ¨le ${currentLanguage.toUpperCase()}`, {
          description: "PremiÃ¨re utilisation, veuillez patienter..."
        });
        await voskModelManager.loadModel(currentLanguage);
      }

      // Simuler l'initialisation du worker Vosk
      // Dans une vraie implÃ©mentation, on crÃ©erait un Worker avec vosk-browser
      console.log(`ðŸŽ¤ Vosk initialisÃ© pour ${currentLanguage}${vadEnabled ? ' avec VAD' : ''}`);
      setEngineStatus('ready');
      
      toast.success(`Vosk prÃªt en ${currentLanguage.toUpperCase()}`, {
        description: `Reconnaissance vocale offline${vadEnabled ? ' avec dÃ©tection automatique' : ''} disponible`
      });
    } catch (error) {
      console.error('Erreur initialisation Vosk:', error);
      setEngineStatus('error');
      toast.error("Erreur Vosk", {
        description: "Impossible d'initialiser Vosk. Utilisez Web Speech API."
      });
    }
  }, [currentEngine, currentLanguage, vadEnabled]);

  useEffect(() => {
    if (currentEngine === 'vosk') {
      initializeVosk();
    }
  }, [currentEngine, initializeVosk]);

  const toggleListening = useCallback(() => {
    if (engineStatus === 'loading') {
      toast.warning("Moteur en cours de chargement", {
        description: "Veuillez patienter..."
      });
      return;
    }

    if (engineStatus === 'error') {
      toast.error("Moteur non disponible", {
        description: "Changez de moteur ou rafraÃ®chissez la page"
      });
      return;
    }

    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  }, [isListening, engineStatus]);

  const startListening = useCallback(async () => {
    if (currentEngine === 'web-speech' && webSpeechRef.current) {
      try {
        webSpeechRef.current.start();
        setIsListening(true);
        console.log(`ðŸŽ¤ Web Speech dÃ©marrÃ© en ${currentLanguage}${vadEnabled ? ' (VAD disponible)' : ''}`);
        
        // DÃ©marrer VAD en parallÃ¨le si activÃ©
        if (vadEnabled && vadSupported && !vadListening) {
          await startVAD();
        }
      } catch (error) {
        console.error('Erreur dÃ©marrage Web Speech:', error);
        toast.error("Erreur dÃ©marrage", {
          description: "Impossible de dÃ©marrer Web Speech API"
        });
      }
    } else if (currentEngine === 'vosk') {
      setIsListening(true);
      console.log(`ðŸŽ¤ Vosk + VAD dÃ©marrÃ© en ${currentLanguage}`);
      
      if (vadEnabled && vadSupported) {
        const success = await startVAD();
        if (!success) {
          setIsListening(false);
          return;
        }
        
        toast.success("Vosk + VAD actif", {
          description: "Parlez naturellement, les segments sont dÃ©tectÃ©s automatiquement"
        });
      } else {
        // Fallback sans VAD pour Vosk (ancienne mÃ©thode)
        setTimeout(() => {
          const mockResults = {
            fr: ["Bonjour, comment allez-vous ?", "Pouvez-vous m'aider ?", "Merci beaucoup"],
            ar: ["Ù…Ø±Ø­Ø¨Ø§ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ", "Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠØŸ", "Ø´ÙƒØ±Ø§ Ø¬Ø²ÙŠÙ„Ø§"]
          };
          const randomResult = mockResults[currentLanguage][Math.floor(Math.random() * mockResults[currentLanguage].length)];
          onResult(randomResult);
          setIsListening(false);
        }, 3000);
      }
    }
  }, [currentEngine, currentLanguage, onResult, vadEnabled, vadSupported, vadListening, startVAD]);

  const stopListening = useCallback(() => {
    if (currentEngine === 'web-speech' && webSpeechRef.current) {
      webSpeechRef.current.stop();
    }
    
    // ArrÃªter VAD si actif
    if (vadEnabled && vadListening) {
      stopVAD();
    }
    
    setIsListening(false);
    console.log(`ðŸ›‘ ${currentEngine} ${vadEnabled ? '+ VAD' : ''} arrÃªtÃ©`);
  }, [currentEngine, vadEnabled, vadListening, stopVAD]);

  const switchEngine = useCallback((engine: SpeechEngine) => {
    if (isListening) {
      stopListening();
    }
    setCurrentEngine(engine);
    console.log(`ðŸ”„ Moteur changÃ© vers: ${engine}${vadEnabled ? ' (VAD activÃ©)' : ''}`);
  }, [isListening, stopListening, vadEnabled]);

  const switchLanguage = useCallback((language: SupportedLanguage) => {
    if (isListening) {
      stopListening();
    }
    setCurrentLanguage(language);
    console.log(`ðŸŒ Langue changÃ©e vers: ${language}`);
  }, [isListening, stopListening]);

  const toggleVAD = useCallback(() => {
    if (isListening) {
      toast.warning("ArrÃªtez l'Ã©coute d'abord", {
        description: "Impossible de changer VAD pendant l'Ã©coute"
      });
      return;
    }
    
    setVadEnabled(!vadEnabled);
    console.log(`ðŸŽ¯ VAD ${!vadEnabled ? 'activÃ©' : 'dÃ©sactivÃ©'}`);
    
    toast.success(`VAD ${!vadEnabled ? 'activÃ©' : 'dÃ©sactivÃ©'}`, {
      description: !vadEnabled 
        ? "DÃ©tection automatique des segments vocaux" 
        : "DÃ©tection manuelle classique"
    });
  }, [vadEnabled, isListening]);

  const getEngineInfo = useCallback(() => {
    const webSpeechSupported = 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window;
    const voskModelLoaded = voskModelManager.isModelLoaded(currentLanguage);
    
    return {
      webSpeech: {
        supported: webSpeechSupported,
        available: webSpeechSupported,
        description: `Reconnaissance en ligne via le navigateur${vadEnabled ? ' + VAD' : ''}`
      },
      vosk: {
        supported: true,
        available: voskModelLoaded,
        description: `Reconnaissance offline privÃ©e${vadEnabled ? ' + dÃ©tection automatique' : ''}`,
        modelProgress: modelProgress.find(p => p.language.includes(currentLanguage === 'fr' ? 'FranÃ§ais' : 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©'))
      },
      vad: {
        supported: vadSupported,
        enabled: vadEnabled,
        status: vadListening ? 'listening' : 'ready',
        bufferStatus
      }
    };
  }, [currentLanguage, modelProgress, vadSupported, vadEnabled, vadListening, bufferStatus]);

  return {
    isListening,
    toggleListening,
    currentEngine,
    currentLanguage,
    switchEngine,
    switchLanguage,
    engineStatus,
    engineInfo: getEngineInfo(),
    modelProgress,
    isSupported: currentEngine === 'web-speech' ? 
      'webkitSpeechRecognition' in window || 'SpeechRecognition' in window :
      true,
    // Nouvelles fonctionnalitÃ©s VAD
    vadEnabled,
    toggleVAD,
    vadSupported,
    vadListening,
    bufferStatus
  };
};
